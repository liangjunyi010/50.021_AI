{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbb8f6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.11/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from opencv-python) (1.26.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install opencv-python\n",
    "!pip install transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adae2e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Headline  Body ID     Stance  \\\n",
      "0  Police find mass graves with at least '15 bodi...      712  unrelated   \n",
      "1  Hundreds of Palestinians flee floods in Gaza a...      158      agree   \n",
      "2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated   \n",
      "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated   \n",
      "4  Spider burrowed through tourist's stomach and ...     1923   disagree   \n",
      "\n",
      "                                         articleBody  \n",
      "0  Danny Boyle is directing the untitled film\\n\\n...  \n",
      "1  Hundreds of Palestinians were evacuated from t...  \n",
      "2  30-year-old Moscow resident was hospitalized w...  \n",
      "3  (Reuters) - A Canadian soldier was shot at the...  \n",
      "4  Fear not arachnophobes, the story of Bunbury's...  \n"
     ]
    }
   ],
   "source": [
    "train_bodies = pd.read_csv('train_bodies.csv')\n",
    "train_stances = pd.read_csv('train_stances.csv')\n",
    "\n",
    "# Merge datasets based on Body ID\n",
    "merged_data = pd.merge(train_stances, train_bodies, on='Body ID')\n",
    "\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66d2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define special token to separate headline and body text\n",
    "special_token = \"[SEP]\"\n",
    "\n",
    "# Concatenate headline and body text with special token\n",
    "combined_texts = merged_data['Headline'] + \" \" + special_token + \" \" + merged_data['articleBody']\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "max_length = 128  \n",
    "\n",
    "for text in combined_texts:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=max_length, \n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt',\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22157b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to integers\n",
    "label_map = {'unrelated': 0, 'discuss': 1, 'agree': 2, 'disagree': 3}\n",
    "merged_data['Stance'] = merged_data['Stance'].map(label_map)\n",
    "labels = torch.tensor(merged_data['Stance'].values)\n",
    "\n",
    "train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,  \n",
    "    sampler=RandomSampler(train_dataset), \n",
    "    batch_size=batch_size \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9eee2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Average Training Loss: 0.2323\n",
      "Epoch 2/20\n",
      "Average Training Loss: 0.0764\n",
      "Epoch 3/20\n",
      "Average Training Loss: 0.0413\n",
      "Epoch 4/20\n",
      "Average Training Loss: 0.0239\n",
      "Epoch 5/20\n",
      "Average Training Loss: 0.0141\n",
      "Epoch 6/20\n",
      "Average Training Loss: 0.0126\n",
      "Epoch 7/20\n",
      "Average Training Loss: 0.0086\n",
      "Epoch 8/20\n",
      "Average Training Loss: 0.0079\n",
      "Epoch 9/20\n",
      "Average Training Loss: 0.0066\n",
      "Epoch 10/20\n",
      "Average Training Loss: 0.0049\n",
      "Epoch 11/20\n",
      "Average Training Loss: 0.0046\n",
      "Epoch 12/20\n",
      "Average Training Loss: 0.0032\n",
      "Epoch 13/20\n",
      "Average Training Loss: 0.0026\n",
      "Epoch 14/20\n",
      "Average Training Loss: 0.0022\n",
      "Epoch 15/20\n",
      "Average Training Loss: 0.0019\n",
      "Epoch 16/20\n",
      "Average Training Loss: 0.0015\n",
      "Epoch 17/20\n",
      "Average Training Loss: 0.0010\n",
      "Epoch 18/20\n",
      "Average Training Loss: 0.0006\n",
      "Epoch 19/20\n",
      "Average Training Loss: 0.0007\n",
      "Epoch 20/20\n",
      "Average Training Loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 20\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        \n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    \n",
    "        loss = torch.nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print(f'Average Training Loss: {avg_train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20652669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Test Data\n",
    "test_bodies = pd.read_csv('competition_test_bodies.csv')\n",
    "test_stances = pd.read_csv('competition_test_stances.csv')\n",
    "\n",
    "merged_test_data = pd.merge(test_stances, test_bodies, on='Body ID')\n",
    "\n",
    "special_token = \"[SEP]\"\n",
    "\n",
    "test_combined_texts = merged_test_data['Headline'] + \" \" + special_token + \" \" + merged_test_data['articleBody']\n",
    "\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "max_length = 128  \n",
    "\n",
    "for text in test_combined_texts:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=max_length, \n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt',\n",
    "                   )\n",
    "    \n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "\n",
    "label_map = {'unrelated': 0, 'discuss': 1, 'agree': 2, 'disagree': 3}\n",
    "merged_test_data['Stance'] = merged_test_data['Stance'].map(label_map)\n",
    "test_labels = torch.tensor(merged_test_data['Stance'].values)\n",
    "\n",
    "# Create DataLoader for Test Data\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)  \n",
    "\n",
    "batch_size = 32\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f78674a-8b72-4382-b32a-3ff9de9a72bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Evaluation Score: 8844.5\n",
      "Total Evaluation Score: 8844.5\n",
      "Average Test Loss: 0.7676\n",
      "Accuracy per Class:\n",
      "Class 0: 0.9905\n",
      "Class 1: 0.8448\n",
      "Class 2: 0.6448\n",
      "Class 3: 0.3113\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     18349\n",
      "           1       0.81      0.84      0.83      4464\n",
      "           2       0.66      0.64      0.65      1903\n",
      "           3       0.51      0.31      0.39       697\n",
      "\n",
      "    accuracy                           0.92     25413\n",
      "   macro avg       0.74      0.70      0.71     25413\n",
      "weighted avg       0.92      0.92      0.92     25413\n",
      "\n",
      "Evaluation Score: 8844.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0\n",
    "predictions = []\n",
    "all_test_labels = []\n",
    "evaluation_score = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        test_input_ids, test_attention_mask, test_labels = batch\n",
    "\n",
    "        test_input_ids = test_input_ids.to(device)\n",
    "        test_attention_mask = test_attention_mask.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "\n",
    "        outputs = model(input_ids=test_input_ids, attention_mask=test_attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits, test_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted_labels = torch.max(logits, dim=1)\n",
    "        predictions.extend(predicted_labels.cpu().numpy())\n",
    "        all_test_labels.extend(test_labels.cpu().numpy())\n",
    "\n",
    "        for true_label, pred_label in zip(test_labels, predicted_labels):\n",
    "            if true_label == pred_label:\n",
    "                evaluation_score += 0.25  # Increase score for any match\n",
    "                if true_label != 0:  # If the label is not 'unrelated' (index 0)\n",
    "                    evaluation_score += 0.50  # Increase score further for non-'unrelated' matches\n",
    "            elif true_label in [1, 2, 3] and pred_label in [1, 2, 3]:  # Check if both are related\n",
    "                evaluation_score += 0.25  # Increase score for related but not exact match\n",
    "\n",
    "# Print the total evaluation score\n",
    "print('Total Evaluation Score:', evaluation_score)\n",
    "\n",
    "# Print the total evaluation score\n",
    "print('Total Evaluation Score:', evaluation_score)\n",
    "\n",
    "# Calculate average test loss\n",
    "avg_test_loss = total_loss / len(test_dataloader)\n",
    "\n",
    "# Calculate accuracy for each class separately\n",
    "accuracy_per_class = {}\n",
    "for class_label in range(4):  # Assuming 4 classes\n",
    "    class_indices = [i for i, label in enumerate(all_test_labels) if label == class_label]\n",
    "    class_predictions = [predictions[i] for i in class_indices]\n",
    "    class_labels = [all_test_labels[i] for i in class_indices]\n",
    "    accuracy_per_class[class_label] = accuracy_score(class_labels, class_predictions)\n",
    "\n",
    "classification_rep = classification_report(all_test_labels, predictions)\n",
    "\n",
    "print(f'Average Test Loss: {avg_test_loss:.4f}')\n",
    "print('Accuracy per Class:')\n",
    "for class_label, accuracy in accuracy_per_class.items():\n",
    "    print(f'Class {class_label}: {accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(classification_rep)\n",
    "print('Evaluation Score:', evaluation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b99e3-821d-4984-b06e-85fbd321ebdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
